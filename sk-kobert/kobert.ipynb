{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQCdqTGpWmGm"
   },
   "source": [
    "## 사전설치\n",
    "* colab\n",
    "* korbert 모델을 사전에 설치하는 작업이 있습니다.\n",
    "* colab에서 !git clone https://github.com/monologg/KoBERT-NER 을 실행하면 drive폴더와 같은 위치에 KoBERT-NER 폴더가 생성됩니다.\n",
    "* KoBERT-NER폴더를 google-drive에 옮긴 다음 폴더의 이름을 바꾸는 작업이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9qaG7VGdE_8"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 설치 후 원하는 google-drive 폴더에 저장\n",
    "# !git clone https://github.com/monologg/KoBERT-NER \n",
    "\n",
    "import os\n",
    "# 저장한 위치를 지정\n",
    "path = '/content/drive/My Drive/Colab Notebooks/'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw-3MFn7ft23"
   },
   "outputs": [],
   "source": [
    "filename = 'KoBERT-NER'\n",
    "cName = 'KoBERT_NER'\n",
    "\n",
    "# kobert호출 경로 지정\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "# 처음 kobert를 설치할 때만 실행\n",
    "# KoBERT-NER 폴더 이름을 KoBERT_NER 으로 변경하는 작업\n",
    "#os.rename(path+filename, path+cName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A20vVJSMdMV4"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "import logging\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "import sentencepiece as spm\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from transformers import BertModel\n",
    "from KoBERT_NER.tokenization_kobert  import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOSs-meofilH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ET8E1NjdfM-"
   },
   "outputs": [],
   "source": [
    "# 모델과 토크나이저를 호출\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-WEajHde-yd"
   },
   "outputs": [],
   "source": [
    "# 사전에 구글드라이브에 데이터를 올려놔야함\n",
    "train = pd.read_csv(\"train_data.txt\", sep='?',encoding = 'euc-kr')\n",
    "test = pd.read_csv(\"test_data.txt\",sep='?',encoding = 'euc-kr')\n",
    "\n",
    "df = pd.concat([train,test]).reset_index()\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTEnHCVke-0-"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df['발화개요'] = df['발화개요'].str.replace(r'[=+,#/\\?:^$.@*\\\"※~&%ㆍ!』○\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\\\n\\t]+', \" \", regex=True)\n",
    "df['발화개요'] = df['발화개요'].str.replace(r'[남,여] [0-9]{1,2}세','',regex=True)\n",
    "df['발화개요'] = df['발화개요'].str.replace(r'[남,여][0-9]{1,2}세','',regex=True)\n",
    "df['발화개요'] = df['발화개요'].str.replace(r'[남,여] [0-9]{1,2}년생','',regex=True)\n",
    "df['발화개요'] = df['발화개요'].str.replace(r'[남,여][0-9]{1,2}년생','',regex=True)\n",
    "df['발화개요'] = df['발화개요'].str.replace('  ','',regex=True)\n",
    "\n",
    "\n",
    "# 데이터 크기가 35보다 적은 것들 탐색\n",
    "drop_index = df[df['발화개요'].str.len() < 18].index\n",
    "df = df.drop(drop_index)\n",
    "labels = df['화재유형'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAd2V9IOe-3F"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qbyonj1ze-7A"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-51f41d5880d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# train 데이터를 버트 인풋에 맞게 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# train_x = [tokens, masks, segments] , train_y = targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# 문장을 bert 학습용 데이터 형태 [토큰, mask, segment], targer으로 분류\n",
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    \n",
    "    SEQ_LEN = 512 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
    "    \n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # 문장 토큰화 및 padding\n",
    "        token = tokenizer.encode(data_df.iloc[i,2], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "       \n",
    "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        \n",
    "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # 정답(0~5)을 targets 변수에 저장해 줌\n",
    "        targets.append(data_df.iloc[i,1])\n",
    "\n",
    "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df['발화개요'] = data_df['발화개요'].astype(str)\n",
    "    data_df['화재유형'] = data_df['화재유형'].astype(int)\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "    return data_x, data_y\n",
    "\n",
    "SEQ_LEN = 512\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "\n",
    "# train 데이터를 버트 인풋에 맞게 변환\n",
    "# train_x = [tokens, masks, segments] , train_y = targets\n",
    "train_x, train_y = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjOao8zQe-9m"
   },
   "outputs": [],
   "source": [
    "# tpu 사용\n",
    "esolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTJQpi_tAJoS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rq55VbS1e-_y"
   },
   "outputs": [],
   "source": [
    "def create_sentiment_bert():\n",
    "  # 버트 pretrained 모델 로드\n",
    "    model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
    "  # 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
    "    token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "    mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
    "    segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
    "  # 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
    "    bert_outputs = model([token_inputs, mask_inputs, segment_inputs])\n",
    "\n",
    "    bert_outputs = bert_outputs[1]\n",
    "    sentiment_first = tf.keras.layers.Dense(6, activation='softmax', kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02))(bert_outputs)\n",
    "    sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
    "  # 옵티마이저는 간단하게 Adam 옵티마이저 활용\n",
    "  # 123456 # 5e-6에서는 0.56에서 정확도가 올라가지 않음\n",
    "    sentiment_model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "    return sentiment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALz8Lg4qe_CE"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "# TPU를 활용하기 위해 context로 묶어주기\n",
    "# TPU 사용해서 모델학습 및 저장\n",
    "# tpu 사용하지 않을 시 with strategy.scope():를 제거\n",
    "with strategy.scope():\n",
    "  \n",
    "    sentiment_model = create_sentiment_bert()\n",
    "    sentiment_model.fit(train_x, train_y, epochs=3, shuffle=False, batch_size=20)\n",
    "    sentiment_model.save_weights(os.path.join(path,\"sentiment_model.h5\"))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch2Dl3Jse_Eo"
   },
   "outputs": [],
   "source": [
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "\n",
    "        token = tokenizer.encode(data_df.iloc[i,2 ], max_length=SEQ_LEN, pad_to_max_length=True)\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        tokens.append(token)\n",
    "        segments.append(segment)\n",
    "        masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "SEQ_LEN = 512\n",
    "DATA_COLUMN = 'data'\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "def predict_load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df['발화개요'] = data_df['발화개요'].astype(str)\n",
    "    data_x = predict_convert_data(data_df)\n",
    "    return data_x\n",
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU9hlV3me_HA"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "mod = sys.modules[__name__]\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    sentiment_model = create_sentiment_bert()\n",
    "  \n",
    "    sentiment_model.load_weights(os.path.join(path,\"sentiment_model.h5\"))\n",
    "    setattr(mod, 'model', sentiment_model)\n",
    "    setattr(mod, 'pred_', sentiment_model.predict(test_set, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "417WXPUSe_JN"
   },
   "outputs": [],
   "source": [
    "def mean_answer_label(*preds):\n",
    "    preds_sum = np.zeros(preds[0].shape[0])\n",
    "    for pred in preds:\n",
    "        preds_sum += np.argmax(pred, axis=-1)\n",
    "    return np.round(preds_sum/len(preds), 0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U0VrSUtpYFg"
   },
   "outputs": [],
   "source": [
    "pretict = mean_answer_label(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1Vn9pTfrpXO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_check(data):\n",
    "    sum = 0\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,1] == pretict[i]:\n",
    "            sum += 1\n",
    "    return sum/len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdU2M4ch5QD2"
   },
   "outputs": [],
   "source": [
    "test_check(test)\n",
    "pretict = list(pretict)\n",
    "ans = test.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyPPgUoRkmqR"
   },
   "outputs": [],
   "source": [
    "fi_score(ans, pretict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7wXSbDj1YUe"
   },
   "source": [
    "epoch = 3\n",
    "train = 0.84 , test = 0.81\n",
    "\n",
    "epoch = 6\n",
    "train = 0.91 , test = 0.8526\n",
    "\n",
    "epoch = 10\n",
    "train = 0.94 , test = 0.849\n",
    "\n",
    "epoch = 15\n",
    "train = 0.97, test = 0.863\n",
    "\n",
    "epoch = 20\n",
    "train = 0.98, test = 0.859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNEcXbthuuVr"
   },
   "outputs": [],
   "source": [
    "def fi_score(test, predict):\n",
    "\n",
    "    if len(test) != len(predict):\n",
    "        return print('예측과 정답의 길이가 다름')\n",
    "\n",
    "    f1_df = [[0 for i in range(len(set(test)))] for i in range(len(set(test)))]\n",
    "\n",
    "        \n",
    "    for i in range(len(test)):\n",
    "        colums = predict[i]\n",
    "        idx =  test[i]\n",
    "        f1_df[idx][colums] += 1\n",
    "\n",
    "    recall = 0\n",
    "    for i in range(len(set(test))):\n",
    "        if sum(f1_df[i]) != 0 :\n",
    "            recall +=f1_df[i][i]/sum(f1_df[i])\n",
    "        else:\n",
    "            recall += 0\n",
    "\n",
    "    precision = 0\n",
    "    for i in range(len(set(test))):\n",
    "        \n",
    "        total = 0\n",
    "        for j in range(len(set(test))):\n",
    "            total += f1_df[j][i]\n",
    "\n",
    "        if total != 0 :\n",
    "            precision +=f1_df[i][i]/total\n",
    "            \n",
    "        else:\n",
    "            precision += 0\n",
    " \n",
    "    recall = recall/6\n",
    "    precision = precision/6\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FnThNWTojfFM"
   },
   "outputs": [],
   "source": [
    "a = [i*1000 for i in range(1,27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000,\n",
       " 2000,\n",
       " 3000,\n",
       " 4000,\n",
       " 5000,\n",
       " 6000,\n",
       " 7000,\n",
       " 8000,\n",
       " 9000,\n",
       " 10000,\n",
       " 11000,\n",
       " 12000,\n",
       " 13000,\n",
       " 14000,\n",
       " 15000,\n",
       " 16000,\n",
       " 17000,\n",
       " 18000,\n",
       " 19000,\n",
       " 20000,\n",
       " 21000,\n",
       " 22000,\n",
       " 23000,\n",
       " 24000,\n",
       " 25000,\n",
       " 26000]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPo9KIBCjWwYjEjT9fLQnSi",
   "collapsed_sections": [],
   "mount_file_id": "1o2qX3en9T4DSGt359uGMoNCwrWOo9-M5",
   "name": "kobert.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
