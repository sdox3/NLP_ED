{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MemN.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1TU4odQGAXCUg0eUx43enEWLMwBDqPptC","authorship_tag":"ABX9TyM+CjpyzSkkppoOB/L5VqJl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"s-idPdaEiGeE"},"source":["from tensorflow.keras.utils import get_file\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","import tarfile\n","from nltk import FreqDist\n","from functools import reduce\n","import os\n","import re\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H620aRiliNag"},"source":["path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/'\n","                'babi_tasks_1-20_v1-2.tar.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"if1Dr84CiOuV"},"source":["with tarfile.open(path) as tar:\n"," tar.extractall()\n"," tar.close()\n","\n","DATA_DIR = 'tasks_1-20_v1-2/en-10k'\n","TRAIN_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_train.txt\")\n","TEST_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_test.txt\")\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4823at6RiOw8"},"source":["def read_data(dir):\n","    # 스토리, 질문, 정답\n","    stories, questions, answers = [], [], []\n","    stroy_temp = [] # 스토리 임시저장\n","    lines = open(dir, 'rb')\n","\n","    for line in lines :\n","        line = line.decode('utf-8')\n","        line = line.strip()\n","        idx, text = line.split(\" \",1)\n","\n","        if int(idx) == 1 :\n","            story_temp = []\n","        \n","        # 질문일 시 srtoy_temp를 story에 저장\n","        if \"\\t\" in text:\n","            # 질문과 답은 \"질문 \\t 정답 \\t 숫자\" 형식으로 구성됨\n","            question, answer, _ = text.split(\"\\t\")\n","            stories.append([x for x in story_temp if x])\n","            questions.append(question)\n","            answers.append(answer)\n","        # 스토리일 시 stroy_temp에 저장\n","        else:\n","            story_temp.append(text)\n","    lines.close()\n","    return stories, questions, answers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTmj4kdPiOza"},"source":["train_data = read_data(TRAIN_FILE)\n","test_data = read_data(TEST_FILE)\n","train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n","test_stories, test_questions, test_answers = read_data(TEST_FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jF946ErWYSjo"},"source":["train_data[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgBkmeokiO1w"},"source":["print('훈련용 스토리의 개수 :', len(train_stories))\n","print('훈련용 질문의 개수 :',len(train_questions))\n","print('훈련용 답변의 개수 :',len(train_answers))\n","print('테스트용 스토리의 개수 :',len(test_stories))\n","print('테스트용 질문의 개수 :',len(test_questions))\n","print('테스트용 답변의 개수 :',len(test_answers))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fRRSDpniO4N"},"source":["def tokenize(sent):\n","    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n","def preprocess_data(train_data, test_data):\n","    counter = FreqDist()\n","\n","    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n","    flatten = lambda data: reduce(lambda x, y: x + y, data)\n","\n","    # 각 샘플의 길이를 저장하는 리스트\n","    story_len = []\n","    question_len = []\n","\n","    for stories, questions, answers in [train_data, test_data]:\n","        for story in stories:\n","            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n","            story_len.append(len(stories)) # 각 story의 길이 저장\n","            for word in stories: # 단어 집합에 단어 추가\n","                counter[word] += 1\n","        for question in questions:\n","            question = tokenize(question)\n","            question_len.append(len(question))\n","            for word in question:\n","                counter[word] += 1\n","        for answer in answers:\n","            answer = tokenize(answer)\n","            for word in answer:\n","                counter[word] += 1\n","\n","    # 단어 집합 생성\n","    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n","    idx2word = {idx : word for word, idx in word2idx.items()}\n","\n","    # 가장 긴 샘플의 길이\n","    story_max_len = np.max(story_len)\n","    question_max_len = np.max(question_len)\n","\n","    return word2idx, idx2word, story_max_len, question_max_len\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ryez9OhniO6j"},"source":["word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7ZxSirtiO_t"},"source":["vocab_size = len(word2idx) + 1\n","print('스토리의 최대 길이 :',story_max_len)\n","print('질문의 최대 길이 :',question_max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvDtgrC4UFo4"},"source":["# 문장의 토큰화 뒤 패팅\n","def vectorize(data, word2idx, story_maxlen, question_maxlen):\n","    Xs, Xq, Y = [], [], []\n","    flatten = lambda data: reduce(lambda x, y: x + y, data)\n","\n","    stories, questions, answers = data\n","    for story, question, answer in zip(stories, questions, answers):\n","        xs = [word2idx[w] for w in tokenize(flatten(story))]\n","        xq = [word2idx[w] for w in tokenize(question)]\n","        Xs.append(xs)\n","        Xq.append(xq)\n","        # 대답이 단답이기 떄문에 for문 안쓰는듯\n","        Y.append(word2idx[answer])\n","\n","        # 스토리와 질문은 각각의 최대 길이로 패딩\n","        # 정답은 원-핫 인코딩\n","    return pad_sequences(Xs, maxlen=story_maxlen),\\\n","           pad_sequences(Xq, maxlen=question_maxlen),\\\n","           to_categorical(Y, num_classes=len(word2idx) + 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSQL9w0kVKkn"},"source":["Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n","Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1ySVgdpVM1f"},"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import Permute, dot, add, concatenate\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhb8WkbxVN63"},"source":["# 에포크 횟수\n","train_epochs = 120\n","# 배치 크기\n","batch_size = 32\n","# 임베딩 크기\n","embed_size = 50\n","# LSTM의 크기\n","lstm_size = 64\n","# 과적합 방지 기법인 드롭아웃 적용 비율\n","dropout_rate = 0.30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"933oZUpvVtIg"},"source":["# story, question input\n","input_sequence = Input((story_max_len,))\n","question = Input((question_max_len,))\n","\n","# story, question input\n","input_sequence = Input((story_max_len,))\n","question = Input((question_max_len,))\n","\n","# story_m\n","input_encoder_m = Sequential()\n","input_encoder_m.add(Embedding(input_dim = vocab_size, output_dim= embed_size))\n","input_encoder_m.add(Dropout(dropout_rate))\n","\n","\n","# story_a\n","input_encoder_c = Sequential()\n","input_encoder_c.add(Embedding(input_dim=vocab_size,\n","                              output_dim=question_max_len))\n","input_encoder_c.add(Dropout(dropout_rate))\n","\n","# question\n","question_encoder = Sequential()\n","question_encoder.add(Embedding(input_dim=vocab_size,\n","                               output_dim=embed_size,\n","                               input_length=question_max_len))\n","question_encoder.add(Dropout(dropout_rate))\n","\n","# sequnetail에 input형식을 넣음\n","input_encoded_m = input_encoder_m(input_sequence)\n","input_encoded_c = input_encoder_c(input_sequence)\n","question_encoded = question_encoder(question)  \n","\n","# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n","# 유사도는 내적(dot)을 사용한다.\n","match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n","match = Activation('softmax')(match)\n","\n","print(match)\n","print(input_encoded_c)\n","# 구한 유사도를 질문에 적용\n","response = add([match, input_encoded_c])  \n","response = Permute((2, 1))(response)  \n","print(response)\n","\n","answer = concatenate([response, question_encoded])\n","print(question_encoded)\n","print(answer)\n","answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n","answer = Dropout(dropout_rate)(answer)\n","answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n","# we output a probability distribution over the vocabulary\n","answer = Activation('softmax')(answer)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atrVqCh4myO5"},"source":["model = Model([input_sequence, question], answer)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit([Xstrain, Xqtrain],\n","         Ytrain, batch_size, train_epochs,\n","         validation_data=([Xstest, Xqtest], Ytest))\n","\n","model.save('MemN_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOgsuMTtdMFF"},"source":["plt.subplot(211)\n","plt.title(\"Accuracy\")\n","plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n","plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n","plt.legend(loc=\"best\")\n","\n","plt.subplot(212)\n","plt.title(\"Loss\")\n","plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n","plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n","plt.legend(loc=\"best\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# labels\n","ytest = np.argmax(Ytest, axis=1)\n","\n","# get predictions\n","Ytest_ = model.predict([Xstest, Xqtest])\n","ytest_ = np.argmax(Ytest_, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02Q_9RGohB4c"},"source":["history.history[\"val_acc\"][-1]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zug3ZZ2QhlXQ"},"source":["history.history[\"val_loss\"][-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzMfHlCxraMI"},"source":[""],"execution_count":null,"outputs":[]}]}